{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "3-Features.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "deletable": true,
        "editable": true,
        "id": "X7HD5UmG5NY-"
      },
      "source": [
        "## Features in computer vision\n",
        "\n",
        "Features are image locations that are \"easy\" to find in the future.  Indeed, one of the early feature detection techniques Lucas-Kanade, sometimes called Kanade-Lucas-Tomasi or KLT features come from a seminal paper called \"Good features to track\".\n",
        "\n",
        "Edges find brightness discontinuities in an image, features find distinctive regions. There are a bunch of different feature detectors and these all have some characteristics in common: they should be quick to find, and things that are close in image-space are close in feature-space (that is, the feature representation of an object looks like the feature representation of objects that look like that object).\n",
        "\n",
        "There is a more in depth *features in OpenCV* set of tutorials [here](https://docs.opencv.org/master/db/d27/tutorial_py_table_of_contents_feature2d.html) and I'll link to various parts of that as appropriate: for more background though, go and work through the whole thing.\n",
        "\n",
        "<p>\n",
        " Estimated time needed: <strong>20 min</strong>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": true,
        "editable": true,
        "id": "_8B7e-Rn5NY_",
        "colab": {}
      },
      "source": [
        "# Download the test image and utils files\n",
        "!wget --no-check-certificate \\\n",
        "    https://raw.githubusercontent.com/computationalcore/introduction-to-opencv/master/assets/noidea.jpg \\\n",
        "    -O noidea.jpg\n",
        "!wget --no-check-certificate \\\n",
        "    https://raw.githubusercontent.com/computationalcore/introduction-to-opencv/master/utils/common.py \\\n",
        "    -O common.py\n",
        "\n",
        "# our usual set of includes\n",
        "# these imports let you use opencv\n",
        "import cv2 #opencv itself\n",
        "import common #some useful opencv functions\n",
        "import numpy as np # matrix manipulations\n",
        "\n",
        "#the following are to do with this interactive notebook code\n",
        "%matplotlib inline \n",
        "from matplotlib import pyplot as plt # this lets you draw inline pictures in the notebooks\n",
        "import pylab # this allows you to control figure size \n",
        "pylab.rcParams['figure.figsize'] = (10.0, 8.0) # this controls figure size in the notebook\n",
        "\n",
        "\n",
        "input_image=cv2.imread('noidea.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "deletable": true,
        "editable": true,
        "id": "yW11cMb15NZB"
      },
      "source": [
        "## Corner detectors\n",
        "If you think of edges as being lines, then corners are an obvious choice for features as they represent the intersection of two lines. One of the earlier corner detectors was introduced by Harris, and it is still a very effective corner detector that gets used quite a lot: it's reliable and it's fast. There's a tutorial explaining how Harris works on the OpenCV site [here](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_features_harris/py_features_harris.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": true,
        "editable": true,
        "id": "JdU2qm635NZC",
        "colab": {}
      },
      "source": [
        "harris_test=input_image.copy()\n",
        "#greyscale it\n",
        "gray = cv2.cvtColor(harris_test,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "gray = np.float32(gray)\n",
        "blocksize=4 # \n",
        "kernel_size=3 # sobel kernel: must be odd and fairly small\n",
        "\n",
        "# run the harris corner detector\n",
        "dst = cv2.cornerHarris(gray,blocksize,kernel_size,0.05) # parameters are blocksize, Sobel parameter and Harris threshold\n",
        "\n",
        "#result is dilated for marking the corners, this is visualisation related and just makes them bigger\n",
        "dst = cv2.dilate(dst,None)\n",
        "#we then plot these on the input image for visualisation purposes, using bright red\n",
        "harris_test[dst>0.01*dst.max()]=[0,0,255]\n",
        "plt.imshow(cv2.cvtColor(harris_test, cv2.COLOR_BGR2RGB))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "Rd8R15G45NZE"
      },
      "source": [
        "Properly speaking the Harris Corner detection is more like a Sobel operator - indeed it is very much like a sobel operator. It doesn't really return a set of features, instead it is a filter which gives a strong response on corner-like regions of the image. We can see this more clearly if we look at the Harris output from the cell above (dst is the Harris response, before thresholding). Well we can kind-of see. You should be able to see that there are slightly light places in the image where there are corner like features, and that there are really light parts of the image around the black and white corners of the writing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": true,
        "editable": true,
        "id": "7z0RTbtO5NZF",
        "colab": {}
      },
      "source": [
        "plt.imshow(dst,cmap = 'gray') \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "0_6wW-nY5NZH"
      },
      "source": [
        "## Moving towards feature space\n",
        "When we consider modern feature detectors there are a few things we need to mention. What makes a good feature includes the following: \n",
        "\n",
        "* Repeatability (got to be able to find it again)\n",
        "* Distinctiveness/informativeness (features representing different things need to be different)\n",
        "* Locality (they need to be local to the image feature and not, like, the whole image)\n",
        "* Quantity (you need to be able to find enough of them for them to be properly useful)\n",
        "* Accuracy (they need to accurately locate the image feature)\n",
        "* Efficiency (they've got to be computable in reasonable time)\n",
        "\n",
        "This comes from a good survey which you can find here (and which I'd thoroughly recommend reading if you're doing feature detection work) [here](https://www.slideshare.net/AhmedOne1/survey-1-project-overview)\n",
        "\n",
        "**Note:** some of the very famous feature detectors (SIFT/SURF and so on) are around, but aren't in OpenCV by default due to patent issues. You can build them for OpenCV if you want - or you can find other implementations (David Lowe's SIFT implementation works just fine). Just google for instructions.  For the purposes of this tutorial (and to save time) we're only going to look at those which are actually in OpenCV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": true,
        "editable": true,
        "id": "DXu8K_FJ5NZI",
        "colab": {}
      },
      "source": [
        "orbimg=input_image.copy()\n",
        "\n",
        "orb = cv2.ORB_create()\n",
        "# find the keypoints with ORB\n",
        "kp = orb.detect(orbimg,None)\n",
        "# compute the descriptors with ORB\n",
        "kp, des = orb.compute(orbimg, kp)\n",
        "# draw keypoints\n",
        "cv2.drawKeypoints(orbimg,kp,orbimg)\n",
        "\n",
        "plt.imshow(cv2.cvtColor(orbimg, cv2.COLOR_BGR2RGB))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "tynAouFx5NZK"
      },
      "source": [
        "## Matching features\n",
        "Finding features is one thing but actually we want to use them for matching. \n",
        "First let's get something where we know there's going to be a match\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": true,
        "editable": true,
        "id": "xyjEKvnK5NZK",
        "colab": {}
      },
      "source": [
        "img2match=np.zeros(input_image.shape,np.uint8)\n",
        "dogface=input_image[60:250, 70:350] # copy out a bit\n",
        "img2match[60:250,70:350]=[0,0,0] # blank that region\n",
        "dogface=cv2.flip(dogface,0) #flip the copy\n",
        "img2match[200:200+dogface.shape[0], 200:200+dogface.shape[1]]=dogface # paste it back somewhere else\n",
        "plt.imshow(cv2.cvtColor(img2match, cv2.COLOR_BGR2RGB))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "deletable": true,
        "editable": true,
        "id": "-pzjjdcD5NZN"
      },
      "source": [
        "## Matching keypoints\n",
        "\n",
        "The feature matching function (in this case Orb) detects and then computes keypoint descriptors. These are a higher dimensional representation of the image region immediately around a point of interest (sometimes literally called \"interest points\"). \n",
        "\n",
        "These higher-dimensional representations can then be matched; the strength you gain from matching these descriptors rather than image regions directly is that they have a certain invariance to transformations (like rotation, or scaling). OpenCV providers matcher routines to do this, in which you can specify the distance measure to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": true,
        "editable": true,
        "id": "a8pk6NeP5NZN",
        "colab": {}
      },
      "source": [
        "\n",
        "kp2 = orb.detect(img2match,None)\n",
        "# compute the descriptors with ORB\n",
        "kp2, des2 = orb.compute(img2match, kp2)\n",
        "# create BFMatcher object: this is a Brute Force matching object\n",
        "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "# Match descriptors.\n",
        "matches = bf.match(des,des2)\n",
        " \n",
        "# Sort them by distance between matches in feature space - so the best matches are first.\n",
        "matches = sorted(matches, key = lambda x:x.distance)\n",
        " \n",
        "# Draw first 50 matches.\n",
        "oimg = cv2.drawMatches(orbimg,kp,img2match,kp2,matches[:50], orbimg)\n",
        " \n",
        "plt.imshow(cv2.cvtColor(oimg, cv2.COLOR_BGR2RGB))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "deletable": true,
        "editable": true,
        "id": "E2qdpBTV5NZP"
      },
      "source": [
        "As you can see there are some false matches, but it's fairly clear that most of the matched keypoints found are actual matches between image regions on the dogface.\n",
        "\n",
        "To be more precise about our matching we could choose to enforce **homography** constraints, which looks for features than sit on the same plane. If you want to investigate that check out this tutorial online\n",
        "[here](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5PUQBH5z5NZS"
      },
      "source": [
        "[Previous](2-Image_stats_and_image_processing.ipynb) [Next](4-Cascade_classification.ipynb)"
      ]
    }
  ]
}